<div align ="center">
  <h1>Ego3D-Bench: Spatial Reasoning with Vision-Language Models in Egocentric Multi-View Scenes</h1>
</div>
<p align="center">
  <img src="assets/sample1.png" width="900" title="">
</p>


<div align="center" style="display: flex; justify-content: center; flex-wrap: wrap; gap: 1em; margin-top: 1em;">

  <a href="https://arxiv.org/abs/your-paper-id" target="_blank" style="background-color: #2e2e2e; color: white; padding: 0.5em 1em; border-radius: 2em; text-decoration: none; font-size: 1em; display: inline-flex; align-items: center;">
    ‚úÇÔ∏è&nbsp; arXiv (Comming Soon!)
  </a>

  <a href="https://github.com/yourusername/yourrepo" target="_blank" style="background-color: #2e2e2e; color: white; padding: 0.5em 1em; border-radius: 2em; text-decoration: none; font-size: 1em; display: inline-flex; align-items: center;">
    üê±&nbsp; Code (Comming Soon!)
  </a>

  <a href="https://huggingface.co/datasets/yourdataset" target="_blank" style="background-color: #2e2e2e; color: white; padding: 0.5em 1em; border-radius: 2em; text-decoration: none; font-size: 1em; display: inline-flex; align-items: center;">
    ü§ó&nbsp; Dataset 
  </a>

</div>
<div align="center">
  <p align="center">
  <h2>Abstract</h2>
Understanding 3D spatial relationships remains a major limitation of current1 Vision-Language Models (VLMs). Prior work has addressed this issue by creating spatial question-answering (QA) datasets based on single images or indoor videos. However, real-world embodied AI agents‚Äîsuch as robots and self-driving cars‚Äîtypically rely on ego-centric, multi-view observations. To this end, we introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial reasoning abilities of VLMs using ego-centric, multi-view outdoor data. Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement from human annotators to ensure quality and diversity. We benchmark 16 SOTA VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results reveal a notable performance gap between human level scores and VLM performance, highlighting that current VLMs still fall short of human level spatial understanding.
  </p>
</div>
<div align ="center">
  <h2>Leaderboard on Ego3D-Bench</h2>
</div>
<div align ="center">
  <h4>Generalist VLMs</h4>
</div>
<p align="center">
  <img src="assets/sample2.png" width="900" title="Leaderboard on Ego3D-Bench">
</p>

<div align ="center">
  <h4>Spatial VLMs</h4>
</div>
<p align="center">
  <img src="assets/sample3.png" width="900" title="Leaderboard on Ego3D-Bench">
</p>
